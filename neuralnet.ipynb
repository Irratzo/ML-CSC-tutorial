{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Neural Network\n",
    "\n",
    "## Introduction\n",
    "\n",
    "An Artificial Neural Network (ANN) is a computational model to approximate complex, non-linear functions Given a set of inputs <b>x</b>, and some internal parameters <b>w</b>, the ANN produces the corresponding outputs <b>y</b> = <i>f</i> (<b>x</b>; <b>w</b>). The ANN is just a clever way of writing <i>f</i>, inspired by the structure of biological brains. The simplest ANN form is sketched below:\n",
    "<table>\n",
    "<tr><td>\n",
    "<img src=\"images/nnscheme.png\" width=\"400px\">\n",
    "</td></tr>\n",
    "<tr><td>\n",
    "Schematic representation of a feed forward neural network.\n",
    "</td></tr>\n",
    "</table><br>\n",
    "The input values are fed to a layer of simple computational units, called <i>neurons</i>, which combine them to calculate their <i>activation</i> output. These values are then sent to the neurons in the output layer, which, in the same way, compute the final output <b>y</b> of the ANN. This is the simplest setup, called feed-forward neural network.\n",
    "<br><br>\n",
    "Each <i>j</i>-th neuron in a layer computes its activation <i>a</i><sub>j</sub> as:\n",
    "<table>\n",
    "<tr><td>\n",
    "<img src=\"images/nnneuron.png\" width=\"400px\">\n",
    "</td></tr>\n",
    "<tr><td>\n",
    "Neuron connection mechanism.\n",
    "</td></tr>\n",
    "</table><br>\n",
    "The parameters <i>w</i><sub>j,i</sub> are connection weights, while <i>b</i><sub>j</sub> are internal neuron biases. All of these parameters make up the ANN parameter set <b>w</b>.\n",
    "The activation function <i>S</i> is arbitrary, as long as it non-linear. Most common activation functions are hyperbolic tangent (used here) and rectified linear unit (ReLU). In some cases, the activation function of output neurons is just linear (weighted sum of inputs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example\n",
    "\n",
    "Here we use the ANN to approximate an analytical function, called Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# --- INITIAL DEFINITIONS ---\n",
    "\n",
    "from pyNN import NeuralNetwork\n",
    "from pyNN import Evaluate\n",
    "import matplotlib.pyplot as plt  # for plotting!\n",
    "import numpy\n",
    "import math\n",
    "import random\n",
    " \n",
    "# This function takes a 2-element vector p=[x,y] and computes z\n",
    "def Model(p):\n",
    "    result = [math.sin(0.4*p[0] - 0.3*p[1] + 0.2*p[0]*p[1])]\n",
    "    result[0] += (2*random.random()-1)*0.1;\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is the model function that will approximate:\n",
    "<img src=\"./images/nnsurface.png\">\n",
    "We added a small random noise every time the function is evaluated, to spice things up!<br><br>\n",
    "\n",
    "Next we create the ANN suitable for this problem. We need two inputs (x,y) and one output (z). The number of hidden neurons is set to 10. The ANN created this way has an initial random set of connection weights and neuron biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# setup the neural network - 2 inputs (x,y), 1 output (z), 10 hidden neurons\n",
    "nn = NeuralNetwork(2, 1, 10)\n",
    "nn.regularization = 0.01 # EXPLAINED LATER!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we use the Model function to creates a dataset for training and validation of the ANN.\n",
    "The training set provides the ANN with a set of <i>examples</i> to learn from, while the validation set is later used to check the quality of the trained ANN.\n",
    "The following code creates a list of (x,y) points on the real plane, with x,y in [-1, 1], to use as inputs.\n",
    "By applying the Model function to each one, we get a list of corresponding correct output z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# training points\n",
    "nTrainPts = 100\n",
    "trainIn = 2*numpy.random.rand(nTrainPts,2) -1 # (x,y) points beween -1 and 1\n",
    "\n",
    "# validation points\n",
    "nValidPts = 40\n",
    "validIn = 2*numpy.random.rand(nValidPts,2)-1\n",
    " \n",
    "# apply Model to the points to compute z for the two sets\n",
    "trainOut = numpy.apply_along_axis(Model, 1, trainIn)\n",
    "validOut = numpy.apply_along_axis(Model, 1, validIn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is what the training set looks like. The colour represents the output z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.scatter(trainIn[:,0], trainIn[:,1], c=trainOut[:,0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training\n",
    "Now comes the tough part! The idea of training is to evaluate the ANN with the training inputs and measure its error (since we know the correct outputs). It is then possible to compute the derivative (gradient) of the error w.r.t. each parameter (connections and biases). By shifting the parameters in the opposite direction of the gradient, we obtain a better set of parameters, that should give smaller error.\n",
    "This procedure can be repeated until the error is minimised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print 'training... '\n",
    "\n",
    "# number of training iterations to perform\n",
    "nEpoch = 100\n",
    "nn.regularization = 0.01\n",
    "\n",
    "# empty array for recording the errors at each step\n",
    "errors = numpy.zeros([nEpoch,2])\n",
    "\n",
    "for x in xrange(nEpoch): #training loop\n",
    "    \n",
    "    # Perform a Gradient Decsent step and get the error on the training set.\n",
    "    # The function takes an array (tensor) of training inputs,\n",
    "    # and the array of their corresponding correct outputs.\n",
    "    # GradStep returns the mean square error (MSE) on the given training set.\n",
    "    error = nn.GradStep(trainIn, trainOut)\n",
    "    # one can also try the stochastic gradient descent\n",
    "    #error = nn.StochasticGradStep(trainIn, trainOut, 20)\n",
    "\n",
    "    # check the error on the validation set\n",
    "    validOutModel = numpy.apply_along_axis(Evaluate, 1, validIn, nn)\n",
    "    errorValid = (validOutModel - validOut) # error\n",
    "    errorValid = errorValid*errorValid      # square error\n",
    "    errorValid = numpy.mean(errorValid.flatten()) # mean square error! (MSE)\n",
    "\n",
    "    # store the errors in the array for later plotting\n",
    "    errors[x,0] = error\n",
    "    errors[x,1] = errorValid\n",
    "\n",
    "print \"training error\", error\n",
    "print \"validation error\", errorValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot the errors\n",
    "plt.plot(errors[:,0], label=\"Training\")\n",
    "plt.plot(errors[:,1], label=\"Validation\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Check the ANN quality with a regression plot, showing the mismatch between the exact and NN predicted outputs for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compute the output on the validation set - again\n",
    "nnout = numpy.apply_along_axis(Evaluate,1,validIn,nn)\n",
    "# compute the error\n",
    "nnerror = (nnout - validOut)\n",
    "\n",
    "plt.plot(validOut[:,0],nnout[:,0],'o')\n",
    "plt.plot([-1,1],[-1,1]) # perfect fit line\n",
    "plt.xlabel('correct output')\n",
    "plt.ylabel('NN output')\n",
    "plt.show()\n",
    "\n",
    "# error histogram\n",
    "plt.hist(nnerror[:,0],50)\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Occurrences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Remarks: regularisation\n",
    "When we created the ANN we set its regularisation property, without explaining it: here is what it's about!<br>\n",
    "In many cases, there are lots of neurons and thus parameters (complex model) but not a lot of data to train them. We might need such complex model for our purpose, but as a rule of thumb there should be at least 10 data points per parameter.<br><br>\n",
    "If not, we are most likely over-fitting the data, i.e. we get an ANN that brilliantly fits the training points, but utterly fails in validation. This is analogous to fitting a bunch of linearly correlated (x,y) points with a 1000-degree polynomial. The fit might go through all the points, but it has no predictive power!<br><br>\n",
    "Simply put, in agreement with Occam, we would like to train the ANN to be as accurante and simple as possible. One way to simplify an ANN with lots of neurons is setting some useless connections to zero.\n",
    "The regularisation term <i>R</i> is a measure of the complexity of the network, for example the sum of squared parameters, that is scaled and added to the output error <i>E</i><sub>data</sub>, to give the total training error <i>E</i> = <i>E</i><sub>data</sub> + <i>a R</i>.<br><br>\n",
    "The gradient of <i>R</i> w.r.t. the parameters steers the training towards smaller weights at the expense of accuracy. The scaling factor for <i>R</i> is the ANN property <b>nn.regularization</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exercise\n",
    "\n",
    "### Wine Classification\n",
    "Use the wine database to train an ANN classifier. Compare the error to the one of K-means method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load the training data - do not edit\n",
    "wineTrainIn = numpy.load(\"./data/wine-training-input.npy\")\n",
    "wineTrainOut = numpy.load(\"./data/wine-training-output.npy\")\n",
    "wineTrainOut = (wineTrainOut - 2) # rescale the output within [-1, 1]\n",
    "\n",
    "# load the validation data - do not edit\n",
    "wineValidIn = numpy.load(\"./data/wine-validation-input.npy\")\n",
    "wineValidOut = numpy.load(\"./data/wine-validation-output.npy\")\n",
    "wineValidOut = (wineValidOut - 2) # rescale the output within [-1, 1]\n",
    "\n",
    "# TODO:\n",
    "# create the ANN\n",
    "# train it\n",
    "# check error - regression plot\n",
    "# estimate amount of misclassified wines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Optional\n",
    "\n",
    "### ANN size\n",
    "Plot the training and evaluation errors dependence on the ANN hidden layer size. Keep the same training and validation set throughout the calculation\n",
    "\n",
    "### Committee machine\n",
    "Train multiple ANN on the same data and form a committee: the output is the average output of the ANN members. Compare the validation performance of the committee to the one of its members.\n",
    "\n",
    "### Combine with PCA - PRO CODER\n",
    "Transform the inputs using PCA and use only the most relevant ones. How is the accuracy of the ANN changing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
