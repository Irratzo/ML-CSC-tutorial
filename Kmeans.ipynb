{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# K-means clustering\n",
    "## Introduction\n",
    "K-means is an example of unsupervised learning algorithms for clustering problems.\n",
    "Suppose we have a data set $X=\\{ \\mathbf{x}_1 \\cdots \\mathbf{x}_N \\}$ consisting of $N$ observations of a random $D$-dimensional Euclidean variable $\\mathbf{x}$. Our goal is to partition the data set into some number $K$ of clusters, located around their centroids $\\mathbf{m}_k=\\{\\mathbf{\\mu}_1 \\cdots \\mathbf{\\mu}_K \\}$. We assume that the value of $K$ is given.\n",
    "\n",
    "K-means clustering can be solved by Expectation Maximisation (EM) algorithm which consists of two steps: E-step and M-step.\n",
    "On each E-step, we find the Euclidean distance between $N$ data points and $K$ cluster centers. The most probable cluster for each data sample $\\mathbf{x}_n$ is the one with nearest centroid:\n",
    "\n",
    "\\begin{align}\n",
    "z_n^*=\\text{arg} \\, \\min\\limits_{k} \\parallel \\mathbf{x}_n - \\mathbf{\\mu}_k \\parallel^2\n",
    "\\end{align}\n",
    "\n",
    "The M-step updates each cluster center by computing the mean of all points assigned to it:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{\\mu}_k=\\frac{1}{N_k} \\sum_{n:z_n = k} \\mathbf{x}_n\n",
    "\\end{align}\n",
    "\n",
    "The pseudo-code of K-means clustering can be summarised as following:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "    Randomly select cluster centers ($\\mathbf{m}_k$) as initial centroids; <br/> \n",
    "<b>while</b> <i>centroids change</i> <b>do</b>:     \n",
    "&emsp; <b>E-step</b>: <br/> \n",
    "&emsp; Calculate the distance between each data point and cluster center (centroid); <br /> \n",
    "&emsp; Assign each data point to its closest cluster center (centroid): $z_n^*=\\text{arg} \\, \\min\\limits_{k} \\parallel \\mathbf{x}_n - \\mathbf{\\mu}_k \\parallel^2$; <br /> \n",
    "&emsp; Form K clusters by assigning each point to its closest centroid; <br /> \n",
    "<br/> \n",
    "&emsp; <b>M-step</b>: <br />\n",
    "&emsp; Update each cluster center by computing the mean of all points assigned to it: $\\mathbf{\\mu}_k=\\frac{1}{N_k} \\sum_{n:z_n = k} \\mathbf{x}_n$; <br /> \n",
    "<b>end</b> <br /> \n",
    "<b>Result</b>: Cluster indices of each data point (assignments)    \n",
    "  </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tutorial 1: Clustering on a simple toy data\n",
    "In this subsection, we implement K-means to cluster a simple toy data. \n",
    "First, we import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We import all necessary libraries\n",
    "from pyKmeans import Kmeans # This imports the Kmeans function (which we created)\n",
    "import numpy as np # This imports numerical python (numpy) library\n",
    "import matplotlib.pyplot as plt # This imports matplotlib library for plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then, we create a simple data based on two independent multivariate Gaussian distributions and we plot this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We create synthetic data using two Gaussian distributions\n",
    "# First, we determine the mean and standard deviation of two Gaussian distributions\n",
    "mu1=np.array([5,2]); sigma1=np.array([[0.4,-0.0255],[-0.0255,0.2]])\n",
    "mu2=np.array([9,7]); sigma2=np.array([[0.10,0],[0,0.4]])\n",
    "\n",
    "# Second, we determine the number of data points on each Gaussian distribution\n",
    "N1=300; N2=100\n",
    "\n",
    "# Third, we add these properties into multivariate normal dist. function in numpy\n",
    "X1=np.random.multivariate_normal(mu1, sigma1, N1)\n",
    "X2=np.random.multivariate_normal(mu2, sigma2, N2)\n",
    "X = np.concatenate((X1, X2), axis=0) # combine X1 and X2 as data X\n",
    "\n",
    "# Fourth, we plot the synthetic data based on two Gaussian distribution function \n",
    "plt.plot(X[:,0],X[:,1],'bo',linewidth=2.0,markersize=4.0)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The result in the Figure shows very well separated data.\n",
    "Of course, it is very easy to visualize the separation/group by human eye, but clustering aims to do this process automatically. K-means clustering is one of simple algorithm to cluster/group data sets.\n",
    "\n",
    "In this tutorial, the number of clusters K is assumed to be known. There are some automatic\n",
    "methods in determining K parameter, but they are not discussed here.\n",
    "\n",
    "We then apply <i>K-means</i> function on the data (if you are interested the detailed of the algorithm, you can take a look at <i>Kmeans.py</i>). The function returns the cluster indices as well as the cluster's\n",
    "centroids. Finally, we plot the data for each cluster using different colour.\n",
    "\n",
    "The produced Figure demonstrates how K-means algorithm successfully forms two clusters in the data.\n",
    "The above problem of course is very easy to visualize because the generated data set is very well separated. The next exercise will demonstrate more challenging problem. The exercise is a good example to understand how K-means can be useful in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# In this part, we will apply K-means clustering algorithm to the above generated data\n",
    "\n",
    "K=2 # determine the number of cluster\n",
    "\n",
    "assignment1, mu_k = Kmeans(X,K)\n",
    "\n",
    "k1=np.argwhere(assignment1==1) # index of cluster 1\n",
    "k2=np.argwhere(assignment1==2) # index of cluster 2\n",
    "\n",
    "plt.plot(X[k1,0],X[k1,1],'ro',X[k2,0],X[k2,1],'go',linewidth=2.0,markersize=4.0)\n",
    "plt.plot(mu_k[0,0],mu_k[0,1],'bx',mu_k[1,0],mu_k[1,1],'bx',linewidth=20.0,markersize=20.0)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Exercise:\n",
    "# Bring the closer the means of Gaussian distribution, what is the effect of K-means clustering\n",
    "# Play around with the generated dataset and see K-means effect\n",
    "# Here we use K=2, because we know we generated the data from two Gaussian distributions, what happened if you change K\n",
    "\n",
    "# Homework:\n",
    "# Understand the K-means clustering function\n",
    "# Change the distance metrics, from Euclidean distance to be Manhattan distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 1: K-means clustering on synthetic data\n",
    "### Exercise 1.a:\n",
    "As instructed earlier in the simple exercise above, vary the aforementioned parameters [mu1,mu2],[sigma1,sigma2] and [N1,N2] and see the plots. You may produce two groups which are not well separated (or at least close each others). Perform K-means clustering on that data.\n",
    "### Exercise 1.b:\n",
    "Create synthetic 3D-data using a multivariate Gaussian distribution, select appropraiately parameters [mu1,mu2,mu3],[sigma1,sigma2,sigma3] and [N1,N2,N3] and see the plots. Perform K-means clustering on that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise 2: K-means clustering on chemical wine data\n",
    "\n",
    "This exercise is a continuation of previous wine exercise (PCA). \n",
    "We will use the wine data set, described in the previous notebook.\n",
    "\n",
    "Based on the eigenvalues generated by PCA in the data, it is known that there are three most dominant PCs. \n",
    "The objective is to apply K-means on the reduced data (using the first three PCs) for clustering three different groups of wine. \n",
    "We know the actual classiffication of the wine from the data set into three types, so we hope that the clusters generated through K-means mimic the actual wine types. \n",
    "\n",
    "### Exercise 2.a:\n",
    "We provide the template for this exercise below.\n",
    "Perform K-means on the reduced data (using only three PCs). Produce a 3D plot of the grouped wine data based on clustering and compare the results with the actual wine classes.\n",
    "\n",
    "### Exercise 2.b:\n",
    "Perform K-means clustering without applying PCA on the data and compare the results with the exercise 2.a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We import all necessary libraries\n",
    "from pyPCA import PCA, zscore   # import the same functions made in the first notebook\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "\n",
    "\n",
    "# We load the wine data again\n",
    "dataIn = np.genfromtxt('./data/wineInputs.txt', delimiter=',')\n",
    "dataOut = np.genfromtxt('./data/wineOutputs.txt', delimiter=',')\n",
    "dataOut=np.int_(dataOut)\n",
    "\n",
    "## # do the zscore and PCA of dataIn\n",
    "#X = ...\n",
    "\n",
    "# Using PCA.py function, apply PCA on the normalized data:\n",
    "#[V, Ypca ,D] = ...\n",
    "\n",
    "\n",
    "# determine the number of cluster\n",
    "K = 3 # We assume that we know the number of cluster\n",
    "\n",
    "## Using Kmeans.py function, apply K-means clustering on the first three PCs data\n",
    "assignment1, mu_k = Kmeans(Ypca[:,0:2],K) # calculate clusters and centroids\n",
    "\n",
    "# Below is to re-show the cumulative sum of PCs\n",
    "noPC=np.linspace(1,len(D),len(D))\n",
    "idc=np.divide(np.cumsum(D),np.sum(D))\n",
    "plt.figure(1)\n",
    "plt.plot(noPC,idc,'bo') # re-plot the data\n",
    "plt.xlabel('no PC')\n",
    "plt.ylabel('cumsum')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# plot the real classes here\n",
    "fig1 = pylab.figure()\n",
    "ax = Axes3D(fig1)\n",
    "ax.scatter(Ypca[dataOut==1,0],Ypca[dataOut==1,1],Ypca[dataOut==1,2],c='b')\n",
    "ax.scatter(Ypca[dataOut==2,0],Ypca[dataOut==2,1],Ypca[dataOut==2,2],c='r')\n",
    "ax.scatter(Ypca[dataOut==3,0],Ypca[dataOut==3,1],Ypca[dataOut==3,2],c='g')\n",
    "plt.title('Real Classes')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Plot the wine classes based on K-means clustering here\n",
    "fig2 = pylab.figure()\n",
    "ax = Axes3D(fig2)\n",
    "ax.scatter(Ypca[assignment1==1,0],Ypca[assignment1==1,1],Ypca[assignment1==1,2],c='b')\n",
    "ax.scatter(Ypca[assignment1==2,0],Ypca[assignment1==2,1],Ypca[assignment1==2,2],c='r')\n",
    "ax.scatter(Ypca[assignment1==3,0],Ypca[assignment1==3,1],Ypca[assignment1==3,2],c='g')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.title('Kmeans')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
